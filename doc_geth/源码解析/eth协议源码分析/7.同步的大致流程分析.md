downloader主要负责区块链最开始的同步工作，当前的同步有两种模式
- 一种是传统的`fullmode`，这种模式通过下载区块头，和区块体来构建区块链，同步的过程就和普通的区块插入的过程一样，包括区块头的验证，交易的验证，交易执行，账户状态的改变等操作，这其实是一个比较消耗CPU和磁盘的一个过程。 
- 另一种模式就是快速同步的`fast sync`模式， 这种模式有专门的文档来描述。请参考fast sync的文档。简单的说 fast sync的模式会下载区块头，区块体和收据， 插入的过程不会执行交易，然后在一个区块高度(最高的区块高度 - 1024)的时候同步所有的账户状态，后面的1024个区块会采用fullmode的方式来构建。 这种模式会加区块的插入时间，同时不会产生大量的历史的账户信息。会相对节约磁盘， 但是对于网络的消耗会更高。 因为需要下载收据和状态。

## downloader 数据结构
```go
type Downloader struct {
	mode atomic.Uint32  // 同步模式定义了使用的策略（每个同步周期），使用d.getMode()来获取SyncMode
	mux  *event.TypeMux // 事件多路复用器用于通告同步操作事件。

	genesis uint64   // 创世块编号用于限制同步范围（例如，轻客户端 CHT）。
	// queue 对象用来调度 区块头，交易，和收据的下载，以及下载完之后的组装
	queue   *queue   // Scheduler for selecting the hashes to download
    // 对端的集合
	peers   *peerSet // Set of active peers from which download can proceed

	stateDB ethdb.Database // 用于状态同步的数据库（并通过去重进行处理）。

	// Statistics 统计信息
	syncStatsChainOrigin uint64       // 同步开始的起始块编号
	syncStatsChainHeight uint64       // 在同步开始时已知的最高块编号
	syncStatsLock        sync.RWMutex // 用于保护同步统计字段的锁

	lightchain LightChain
	blockchain BlockChain

	// Callbacks
	dropPeer peerDropFn // Drops a peer for misbehaving
	badBlock badBlockFn // Reports a block as rejected by the chain

	// Status
	synchroniseMock func(id string, hash common.Hash) error // 用于测试中的同步替代方案。
	synchronising   atomic.Bool
	notified        atomic.Bool
	committed       atomic.Bool
	ancientLimit    uint64 // 可以被视为古老数据的最大块编号。

	// Channels
	headerProcCh chan *headerTask // Channel to feed the header processor new tasks

	// Skeleton sync
	skeleton *skeleton // Header skeleton to backfill the chain with (eth2 mode)

	// State sync
	pivotHeader *types.Header // 用于动态推送同步状态根的中心区块头
	pivotLock   sync.RWMutex  // 用于保护中心区块头读取不受更新影响的锁

	SnapSyncer     *snap.Syncer // TODO(karalabe): make private! hack for now
	stateSyncStart chan *stateSync // 用来启动新的 state fetcher

	// Cancellation and termination
	cancelPeer string         // 当前用作主节点的节点标识符（在丢弃时取消）
	cancelCh   chan struct{}  // 用于取消中途同步的通道
	cancelLock sync.RWMutex   // 用于保护取消通道和节点在传递中的锁
	cancelWg   sync.WaitGroup // 确保所有的获取器 goroutine 已经退出。

	quitCh   chan struct{} // Quit channel to signal termination
	quitLock sync.Mutex    // Lock to prevent double closes

	// Testing hooks
	syncInitHook     func(uint64, uint64)  // 在启动新的同步运行时调用的方法
	bodyFetchHook    func([]*types.Header) // 在开始获取区块体时调用的方法
	receiptFetchHook func([]*types.Header) // 在开始获取收据时调用的方法
	chainInsertHook  func([]*fetchResult)  // 在插入一系列区块（可能需要多次调用）时调用的方法

	// Progress reporting metrics
	syncStartBlock uint64    // Geth启动时的头快照块
	syncStartTime  time.Time // 链同步开始时的时间实例
	syncLogTime    time.Time // 最后一次报告状态的时间实例
}
```

### 构造方法
```go
// New 创建一个新的下载器，用于从远程节点获取哈希和区块。
func New(stateDb ethdb.Database, mux *event.TypeMux, chain BlockChain, lightchain LightChain, dropPeer peerDropFn, success func()) *Downloader {
	if lightchain == nil {
		lightchain = chain
	}
	dl := &Downloader{
		stateDB:        stateDb,
		mux:            mux,
		queue:          newQueue(blockCacheMaxItems, blockCacheInitialItems),
		peers:          newPeerSet(),
		blockchain:     chain,
		lightchain:     lightchain,
		dropPeer:       dropPeer,
		headerProcCh:   make(chan *headerTask, 1),
		quitCh:         make(chan struct{}),
		SnapSyncer:     snap.NewSyncer(stateDb, chain.TrieDB().Scheme()),
		stateSyncStart: make(chan *stateSync),
		syncStartBlock: chain.CurrentSnapBlock().Number.Uint64(),
	}
	// Create the post-merge skeleton syncer and start the process
	// 创建 后合并骨架 同步器 并启动该过程。
	dl.skeleton = newSkeleton(stateDb, dl.peers, dropPeer, newBeaconBackfiller(dl, success))

	// 启动stateFetcher的任务监听，但是这个时候还没有生成state fetcher的任务。
	go dl.stateFetcher()
	return dl
}
```

## 同步下载
`LegacySync`试图和一个peer来同步，如果同步过程中遇到一些错误，那么会删除掉Peer。然后会被重试。
```go
// LegacySync 尝试将本地区块链与远程节点进行同步，同时添加各种健全性检查，并使用各种日志条目进行包装。
func (d *Downloader) LegacySync(id string, head common.Hash, td, ttd *big.Int, mode SyncMode) error {
	err := d.synchronise(id, head, td, ttd, mode, false, nil)

	switch err {
	case nil, errBusy, errCanceled:
		return err
	}
	// 如果是其中一种错误的话
	if errors.Is(err, errInvalidChain) || errors.Is(err, errBadPeer) || errors.Is(err, errTimeout) ||
		errors.Is(err, errStallingPeer) || errors.Is(err, errUnsyncedPeer) || errors.Is(err, errEmptyHeaderSet) ||
		errors.Is(err, errPeersUnavailable) || errors.Is(err, errTooOld) || errors.Is(err, errInvalidAncestor) {
		log.Warn("Synchronisation failed, dropping peer", "peer", id, "err", err)
        // 如果检测到恶意节点并且进行删除操作的回调函数为空
		if d.dropPeer == nil {
			// 当使用 `--copydb` 进行本地复制时，`dropPeer` 方法为空。
			// 如果例如压缩在错误的时间发生，可能会出现超时情况，但可以忽略。
			log.Warn("Downloader wants to drop peer, but peerdrop-function is not set", "peer", id)
		} else {
			// 否则，删除恶意节点
			d.dropPeer(id)
		}
		return err
	}
	if errors.Is(err, ErrMergeTransition) {
		return err // 这是一个预期的错误，请不要在自旋循环中不断打印它。
	}
	log.Warn("Synchronisation failed, retrying", "err", err)
	return err
}
```

synchronise
```go
// synchronise 方法将选择节点并使用它进行同步。
// 如果传入一个空字符串，它将选择最佳节点，并在其总难度高于本地节点时进行同步。
// 如果任何检查失败，将返回错误。此方法是同步的。
func (d *Downloader) synchronise(id string, hash common.Hash, td, ttd *big.Int, mode SyncMode, beaconMode bool, beaconPing chan struct{}) error {
	// beacon header syncer 是异步的。
	// 它会启动此同步过程，并继续执行其他任务。
	// 然而，如果需要取消同步，syncer 需要知道我们是否已经达到了启动点（并初始化了取消通道）或者还没有。
	// 确保即使在出现错误的情况下也会发出信号。
	if beaconPing != nil {
		defer func() {
			select {
			case <-beaconPing: // already notified
			default:
				close(beaconPing) // 奇怪的退出条件，通知可以安全取消（什么也不做）。
			}
		}()
	}
	// 如果进行测试，可以模拟同步过程。
	if d.synchroniseMock != nil {
		return d.synchroniseMock(id, hash)
	}
	// 确保只有一个 goroutine 在此点之后被允许执行。
	if !d.synchronising.CompareAndSwap(false, true) {
		return errBusy
	}
	defer d.synchronising.Store(false)

	// 发布用户通知同步的消息（每个会话只发布一次）。
	if d.notified.CompareAndSwap(false, true) {
		log.Info("Block synchronisation started")
	}
	if mode == SnapSync {
		// 快照同步使用快照命名空间来存储可能不稳定的数据，直到同步完全修复和完成。
		// 在此期间暂停快照维护，以防止访问。
		if snapshots := d.blockchain.Snapshots(); snapshots != nil { // Only nil in tests
			snapshots.Disable()
		}
	}
	// 重置队列、节点集合和唤醒通道，以清除任何内部遗留状态。
	d.queue.Reset(blockCacheMaxItems, blockCacheInitialItems)
	d.peers.Reset()

	// 没看懂这里的作用
	for _, ch := range []chan bool{d.queue.blockWakeCh, d.queue.receiptWakeCh} {
		select {
		case <-ch:
		default:
		}
	}
	// 这里是等待d.headerProcCh通知
	for empty := false; !empty; {
		select {
		case <-d.headerProcCh:
		default:
			empty = true
		}
	}
	// 创建用于中途中止的取消通道，并标记主节点。
	d.cancelLock.Lock()
	d.cancelCh = make(chan struct{})
	d.cancelPeer = id
	d.cancelLock.Unlock()

	defer d.Cancel() // 无论如何，我们都不能让取消通道保持打开状态。

	// 以原子方式设置所请求的同步模式。
	d.mode.Store(uint32(mode))

	// 检索原始节点并启动下载过程。
	var p *peerConnection
	if !beaconMode { // Beacon模式不需要从节点进行同步。
		p = d.peers.Peer(id)
		if p == nil {
			return errUnknownPeer
		}
	}
	if beaconPing != nil {
		close(beaconPing)
	}
	return d.syncWithPeer(p, hash, td, ttd, beaconMode)
}
```

syncWithPeer
```go
// syncWithPeer 根据指定节点和头哈希的哈希链开始块同步。
func (d *Downloader) syncWithPeer(p *peerConnection, hash common.Hash, td, ttd *big.Int, beaconMode bool) (err error) {
	d.mux.Post(StartEvent{})
	defer func() {
		// reset on error
		if err != nil {
			d.mux.Post(FailedEvent{err})
		} else {
			latest := d.lightchain.CurrentHeader()
			d.mux.Post(DoneEvent{latest})
		}
	}()
	mode := d.getMode()

	if !beaconMode {
		log.Debug("Synchronising with the network", "peer", p.id, "eth", p.version, "head", hash, "td", td, "mode", mode)
	} else {
		log.Debug("Backfilling with the network", "mode", mode)
	}
	defer func(start time.Time) {
		log.Debug("Synchronisation terminated", "elapsed", common.PrettyDuration(time.Since(start)))
	}(time.Now())

	// 查找同步边界：共同祖先和目标区块。
	var latest, pivot, final *types.Header
	if !beaconMode {
		// 在传统模式下，使用主节点从中检索标头。
		latest, pivot, err = d.fetchHead(p)
		if err != nil {
			return err
		}
	} else {
		// 在Beacon模式下，使用skeleton chain从中检索标头。
		latest, _, final, err = d.skeleton.Bounds()
		if err != nil {
			return err
		}
		if latest.Number.Uint64() > uint64(fsMinFullBlocks) {
			number := latest.Number.Uint64() - uint64(fsMinFullBlocks)

			// 从 skeleton chain segment 中检索枢轴标头，
			// 如果在 skeleton space 中找不到，则回退到本地链。
			if pivot = d.skeleton.Header(number); pivot == nil {
				_, oldest, _, _ := d.skeleton.Bounds() // error is already checked
				if number < oldest.Number.Uint64() {
					count := int(oldest.Number.Uint64() - number) // 它受到fsMinFullBlocks的限制。
					headers := d.readHeaderRange(oldest, count)
					if len(headers) == count {
						pivot = headers[len(headers)-1]
						log.Warn("Retrieved pivot header from local", "number", pivot.Number, "hash", pivot.Hash(), "latest", latest.Number, "oldest", oldest.Number)
					}
				}
			}
			// 如果仍然找不到枢轴标头，则打印错误日志并直接返回。
			// 这意味着骨架链与本地链没有正确连接。
			if pivot == nil {
				log.Error("Pivot header is not found", "number", number)
				return errNoPivotHeader
			}
		}
	}
	// 如果没有返回枢轴块(pivot block)，则头部低于最小完整块阈值（即新链）。
	// 在这种情况下，我们无论如何都不会进行快速同步，但仍然需要一个有效的枢轴块，
	// 以避免某些代码在访问时出现空指针异常。
	if mode == SnapSync && pivot == nil {
		pivot = d.blockchain.CurrentBlock()
	}
	height := latest.Number.Uint64()

	var origin uint64
	if !beaconMode {
		// 在传统模式下，与网络建立联系并找到共同祖先。
		origin, err = d.findAncestor(p, latest)
		if err != nil {
			return err
		}
	} else {
		// 在Beacon模式下，使用骨架链(skeleton chain)进行共同祖先查找。
		origin, err = d.findBeaconAncestor()
		if err != nil {
			return err
		}
	}
	d.syncStatsLock.Lock()
	if d.syncStatsChainHeight <= origin || d.syncStatsChainOrigin > origin {
		d.syncStatsChainOrigin = origin
	}
	d.syncStatsChainHeight = height
	d.syncStatsLock.Unlock()

	// 确保我们的起始点位于任何快速同步(snap sync)枢轴点之下。
	if mode == SnapSync {
		if height <= uint64(fsMinFullBlocks) {
			origin = 0
		} else {
			pivotNumber := pivot.Number.Uint64()
			if pivotNumber <= origin {
				origin = pivotNumber - 1
			}
			// 将枢轴写入数据库，以便在回滚超过它时重新启用快速同步。
			rawdb.WriteLastPivotNumber(d.stateDB, pivotNumber)
		}
	}
	d.committed.Store(true)
	if mode == SnapSync && pivot.Number.Uint64() != 0 {
		d.committed.Store(false)
	}
	if mode == SnapSync {
		// 设置古老数据限制。如果我们正在运行快速同步，则所有早于ancientLimit的块数据将被写入古老存储。
		// 较新的数据将被写入活动数据库，并等待冷冻器迁移。
		// 
		// 如果网络是合并后的状态，则将最后公布的已完成块作为古老限制，如果我们尚未收到，
		// 则将head作为最大分叉祖先限制。如果我们已经超过了已完成的块，
		// 则skeleton.Bounds将返回nil，我们将恢复到head-90K。
		// 这没关系，我们无论如何都要完成同步。
		// 
		// 对于非合并网络，如果有可用的检查点，则通过该检查点计算ancientLimit。
		// 否则，通过远程节点的广告高度计算ancientLimit。
		// 这主要是为传统网络提供的回退选项，但最终应该被删除。待办事项（karalabe）。
		if beaconMode {
			// 在Beacon同步中，将最新的已完成块作为古老限制，如果尚未公布已完成块，
			// 则使用一个合理的高度作为古老限制。
			if final != nil {
				d.ancientLimit = final.Number.Uint64()
			} else if height > fullMaxForkAncestry+1 {
				d.ancientLimit = height - fullMaxForkAncestry - 1
			} else {
				d.ancientLimit = 0
			}
		} else {
			// 在传统同步中，使用我们从远程节点获得的最佳公告。
			// TODO(karalabe): Drop this pathway.
			if height > fullMaxForkAncestry+1 {
				d.ancientLimit = height - fullMaxForkAncestry - 1
			} else {
				d.ancientLimit = 0
			}
		}
		frozen, _ := d.stateDB.Ancients() // 忽略此处的错误，因为轻客户端也可能会遇到这个问题。

		// 如果区块链数据的一部分已经写入活动存储中，则明确禁用古老风格的插入。
		if origin >= frozen && frozen != 0 {
			d.ancientLimit = 0
			log.Info("Disabling direct-ancient mode", "origin", origin, "ancient", frozen-1)
		} else if d.ancientLimit > 0 {
			log.Debug("Enabling direct-ancient mode", "ancient", d.ancientLimit)
		}
		// 如果发生重组（reorg），则回滚古老存储和区块链。
		if origin+1 < frozen {
			if err := d.lightchain.SetHead(origin); err != nil {
				return err
			}
		}
	}
	// 使用并发的header和内容检索算法启动同步。
	d.queue.Prepare(origin+1, mode)
	if d.syncInitHook != nil {
		d.syncInitHook(origin, height)
	}
	var headerFetcher func() error
	if !beaconMode {
		// In legacy mode, headers are retrieved from the network
		headerFetcher = func() error { return d.fetchHeaders(p, origin+1, latest.Number.Uint64()) }
	} else {
		// In beacon mode, headers are served by the skeleton syncer
		headerFetcher = func() error { return d.fetchBeaconHeaders(origin + 1) }
	}
	fetchers := []func() error{
		headerFetcher, // Headers are always retrieved
		func() error { return d.fetchBodies(origin+1, beaconMode) },   // 在正常同步和快速同步期间检索区块内容。
		func() error { return d.fetchReceipts(origin+1, beaconMode) }, // 在快速同步期间检索收据。
		func() error { return d.processHeaders(origin+1, td, ttd, beaconMode) },
	}
	if mode == SnapSync {
		d.pivotLock.Lock()
		d.pivotHeader = pivot
		d.pivotLock.Unlock()

		fetchers = append(fetchers, func() error { return d.processSnapSyncContent() })
	} else if mode == FullSync {
		fetchers = append(fetchers, func() error { return d.processFullSyncContent(ttd, beaconMode) })
	}
	return d.spawnSync(fetchers)
}
```

spawnSync给每个fetcher启动一个goroutine, 然后阻塞的等待fetcher出错。
```go
// spawnSync函数在单独的goroutines中运行d.process和所有给定的fetcher函数，
// 直到完成，并返回出现的第一个错误。
func (d *Downloader) spawnSync(fetchers []func() error) error {
	errc := make(chan error, len(fetchers))
	d.cancelWg.Add(len(fetchers))
	for _, fn := range fetchers {
		fn := fn
		go func() { defer d.cancelWg.Done(); errc <- fn() }()
	}
	// Wait for the first error, then terminate the others.
	var err error
	for i := 0; i < len(fetchers); i++ {
		if i == len(fetchers)-1 {
			// 当所有fetcher函数退出时，关闭队列。这将导致块处理器在处理完队列后结束。
			d.queue.Close()
		}
		if got := <-errc; got != nil {
			err = got
			if got != errCanceled {
				break // 接收一个有意义的错误，并将其上抛。
			}
		}
	}
	d.queue.Close()
	d.Cancel()
	return err
}
```

## headers的处理
fetchHeaders方法用来获取header。 然后根据获取的header去获取body和receipt等信息。
```go
// fetchHeaders函数在并发地从所请求的数量中持续检索头信息，直到没有更多头信息返回为止，
// 在此过程中可能进行限流。为了促进并发，但仍然防止恶意节点发送错误的头信息，
// 我们使用与正在同步的"origin"节点构建一个头信息链的框架，并使用其他节点填充缺失的头信息。
// 只有当其他节点的头信息与框架完全匹配时，才接受来自其他节点的头信息。
// 如果没有人可以填充框架 - 即使是原始节点也不行 - 则假定它无效，并且将原始节点丢弃。
func (d *Downloader) fetchHeaders(p *peerConnection, from uint64, head uint64) error {
	p.log.Debug("Directing header downloads", "origin", from)
	defer p.log.Debug("Header download terminated")

	// 开始拉取头信息链的框架，直到全部完成。
	var (
		skeleton = true  // 框架组装阶段或结束阶段
		pivoting = false // 下一个请求是否是枢轴验证。
		ancestor = from
		mode     = d.getMode()
	)
	for {
		// 拉取下一批头信息，可能是以下情况之一：
		// - 枢轴检查，以查看链是否移动得太远
		// - 骨架检索，以允许并发地获取头信息
		// - 如果我们接近链的末尾，则进行完整的头信息检索。
		var (
			headers []*types.Header
			hashes  []common.Hash
			err     error
		)
		switch {
		case pivoting:
			d.pivotLock.RLock()
			pivot := d.pivotHeader.Number.Uint64()
			d.pivotLock.RUnlock()

			p.log.Trace("Fetching next pivot header", "number", pivot+uint64(fsMinFullBlocks))
			headers, hashes, err = d.fetchHeadersByNumber(p, pivot+uint64(fsMinFullBlocks), 2, fsMinFullBlocks-9, false) // move +64 when it's 2x64-8 deep

		case skeleton:
			p.log.Trace("Fetching skeleton headers", "count", MaxHeaderFetch, "from", from)
			headers, hashes, err = d.fetchHeadersByNumber(p, from+uint64(MaxHeaderFetch)-1, MaxSkeletonSize, MaxHeaderFetch-1, false)

		default:
			p.log.Trace("Fetching full headers", "count", MaxHeaderFetch, "from", from)
			headers, hashes, err = d.fetchHeadersByNumber(p, from, MaxHeaderFetch, 0, false)
		}
		switch err {
		case nil:
			// 头信息已检索完成，继续进行处理。

		case errCanceled:
			// 同步已取消，没有问题，向上传递。
			return err

		default:
			// 头信息检索超时或对等节点以某种奇怪的方式失败（例如断开连接）。
			// 考虑到主节点有问题，丢弃它。
			d.dropPeer(p.id)

			// 优雅地完成同步，而不是简单地丢弃已收集的数据。
			for _, ch := range []chan bool{d.queue.blockWakeCh, d.queue.receiptWakeCh} {
				select {
				case ch <- false:
				case <-d.cancelCh:
				}
			}
			select {
			case d.headerProcCh <- nil:
			case <-d.cancelCh:
			}
			return fmt.Errorf("%w: header request failed: %v", errBadPeer, err)
		}
		// 如果正在进行枢轴检查，如果枢轴变得过时，则进行移动并运行真正的检索。
		var pivot uint64

		d.pivotLock.RLock()
		if d.pivotHeader != nil {
			pivot = d.pivotHeader.Number.Uint64()
		}
		d.pivotLock.RUnlock()

		if pivoting {
			if len(headers) == 2 {
				if have, want := headers[0].Number.Uint64(), pivot+uint64(fsMinFullBlocks); have != want {
					log.Warn("Peer sent invalid next pivot", "have", have, "want", want)
					return fmt.Errorf("%w: next pivot number %d != requested %d", errInvalidChain, have, want)
				}
				if have, want := headers[1].Number.Uint64(), pivot+2*uint64(fsMinFullBlocks)-8; have != want {
					log.Warn("Peer sent invalid pivot confirmer", "have", have, "want", want)
					return fmt.Errorf("%w: next pivot confirmer number %d != requested %d", errInvalidChain, have, want)
				}
				log.Warn("Pivot seemingly stale, moving", "old", pivot, "new", headers[0].Number)
				pivot = headers[0].Number.Uint64()

				d.pivotLock.Lock()
				d.pivotHeader = headers[0]
				d.pivotLock.Unlock()

				// 将枢轴写入数据库，以便在进行回滚时重新启用快照同步，并更新状态同步器将要下载的状态根。
				rawdb.WriteLastPivotNumber(d.stateDB, pivot)
			}
			// 禁用枢轴检查，并获取下一批头信息。
			pivoting = false
			continue
		}
		// 如果骨架已完成，则直接从原始节点拉取任何剩余的头信息。
		if skeleton && len(headers) == 0 {
			// 恶意节点可能会无限期地保留宣传的头信息。
			if from+uint64(MaxHeaderFetch)-1 <= head {
				p.log.Warn("Peer withheld skeleton headers", "advertised", head, "withheld", from+uint64(MaxHeaderFetch)-1)
				return fmt.Errorf("%w: withheld skeleton headers: advertised %d, withheld #%d", errStallingPeer, head, from+uint64(MaxHeaderFetch)-1)
			}
			p.log.Debug("No skeleton, fetching headers directly")
			skeleton = false
			continue
		}
		// 如果没有更多的头信息传入，请通知内容获取器并返回。
		if len(headers) == 0 {
			// 在下载枢轴时不要中止头信息的获取。
			if !d.committed.Load() && pivot <= from {
				p.log.Debug("No headers, waiting for pivot commit")
				select {
				case <-time.After(fsHeaderContCheck):
					continue
				case <-d.cancelCh:
					return errCanceled
				}
			}
			// 枢轴已完成（或不在快照同步中），且没有更多的头信息，终止进程。
			p.log.Debug("No more headers available")
			select {
			case d.headerProcCh <- nil:
				return nil
			case <-d.cancelCh:
				return errCanceled
			}
		}
		// 如果我们收到了一个框架批次，同时并发解析内部数据。
		var progressed bool
		if skeleton {
			filled, hashset, proced, err := d.fillHeaderSkeleton(from, headers)
			if err != nil {
				p.log.Debug("Skeleton chain invalid", "err", err)
				return fmt.Errorf("%w: %v", errInvalidChain, err)
			}
			headers = filled[proced:]
			hashes = hashset[proced:]

			progressed = proced > 0
			from += uint64(proced)
		} else {
			// 恶意节点可能会无限期地保留宣传的头信息。
			if n := len(headers); n < MaxHeaderFetch && headers[n-1].Number.Uint64() < head {
				p.log.Warn("Peer withheld headers", "advertised", head, "delivered", headers[n-1].Number.Uint64())
				return fmt.Errorf("%w: withheld headers: advertised %d, delivered %d", errStallingPeer, head, headers[n-1].Number.Uint64())
			}
			// 如果我们接近链的末尾，但尚未到达，延迟最后几个头信息，
			// 以便头部的小型重组不会导致无效的哈希链错误。
			if n := len(headers); n > 0 {
				// Retrieve the current head we're at
				var head uint64
				if mode == LightSync {
					head = d.lightchain.CurrentHeader().Number.Uint64()
				} else {
					head = d.blockchain.CurrentSnapBlock().Number.Uint64()
					if full := d.blockchain.CurrentBlock().Number.Uint64(); head < full {
						head = full
					}
				}
				// 如果头部位于共同祖先以下，实际上我们正在去重已存在的链段，
				// 所以将祖先作为虚假的头部。否则，我们可能会无意义地延迟头信息的传递。
				if head < ancestor {
					head = ancestor
				}
				// 如果头部比当前批次要旧得多，延迟最后几个头信息。
				if head+uint64(reorgProtThreshold) < headers[n-1].Number.Uint64() {
					delay := reorgProtHeaderDelay
					if delay > n {
						delay = n
					}
					headers = headers[:n-delay]
					hashes = hashes[:n-delay]
				}
			}
		}
		// 如果没有头信息被传递，或者所有头信息都被延迟了，稍微休眠一下然后重试。
		// 在填充骨架期间已经使用的头信息要小心处理。
		if len(headers) == 0 && !progressed {
			p.log.Trace("All headers delayed, waiting")
			select {
			case <-time.After(fsHeaderContCheck):
				continue
			case <-d.cancelCh:
				return errCanceled
			}
		}
		// 插入任何剩余的新头信息并获取下一批头信息。
		if len(headers) > 0 {
			p.log.Trace("Scheduling new headers", "count", len(headers), "from", from)
			select {
			case d.headerProcCh <- &headerTask{
				headers: headers,
				hashes:  hashes,
			}:
			case <-d.cancelCh:
				return errCanceled
			}
			from += uint64(len(headers))
		}
		// 如果我们仍在进行骨架填充快照同步，
		// 请在继续下一轮骨架填充之前检查枢轴是否过时。
		if skeleton && pivot > 0 {
			pivoting = true
		}
	}
}
```

`processHeaders`方法，这个方法从`headerProcCh`通道来获取header。并把获取到的header丢入到queue来进行调度，这样body fetcher或者是receipt fetcher就可以领取到fetch任务。
```go
// processHeaders函数从输入通道接收检索到的头信息批次，
// 并持续处理和调度这些头信息到头信息链和下载器的队列中，直到流结束或发生故障。
func (d *Downloader) processHeaders(origin uint64, td, ttd *big.Int, beaconMode bool) error {
	// 记录不确定的header以便回滚。
	var (
		rollback    uint64 // 零表示不回滚（因为无法取消创世状态）。
		rollbackErr error
		mode        = d.getMode()
	)
	defer func() {
		if rollback > 0 {
			lastHeader, lastFastBlock, lastBlock := d.lightchain.CurrentHeader().Number, common.Big0, common.Big0
			if mode != LightSync {
				lastFastBlock = d.blockchain.CurrentSnapBlock().Number
				lastBlock = d.blockchain.CurrentBlock().Number
			}
			if err := d.lightchain.SetHead(rollback - 1); err != nil { // -1 to target the parent of the first uncertain block
				// 我们已经在回滚堆栈了，只需打印错误以使其更加明显。
				log.Error("Failed to roll back chain segment", "head", rollback-1, "err", err)
			}
			curFastBlock, curBlock := common.Big0, common.Big0
			if mode != LightSync {
				curFastBlock = d.blockchain.CurrentSnapBlock().Number
				curBlock = d.blockchain.CurrentBlock().Number
			}
			log.Warn("Rolled back chain segment",
				"header", fmt.Sprintf("%d->%d", lastHeader, d.lightchain.CurrentHeader().Number),
				"snap", fmt.Sprintf("%d->%d", lastFastBlock, curFastBlock),
				"block", fmt.Sprintf("%d->%d", lastBlock, curBlock), "reason", rollbackErr)
		}
	}()
	// 等待批处理的标题进行处理。
	gotHeaders := false

	for {
		select {
		case <-d.cancelCh:
			rollbackErr = errCanceled
			return errCanceled

		case task := <-d.headerProcCh:
			// 如果同步完成，请终止标题处理。
			if task == nil || len(task.headers) == 0 {
				// 通知所有人标题已完全处理。
				for _, ch := range []chan bool{d.queue.blockWakeCh, d.queue.receiptWakeCh} {
					select {
					case ch <- false:
					case <-d.cancelCh:
					}
				}
				// 如果我们处于传统同步模式下，我们需要检查恶意节点引起的总难度违规情况。
				// 在信标模式下，这是不需要的，我们可以跳过检查并终止同步。
				if !beaconMode {
					// 如果根本没有检索到任何标题，那么该节点违反了其承诺的总难度，即它拥有比我们更好的链。
					// 
					// 唯一的例外是如果其承诺的区块已经通过其他方式导入（例如，获取器）：
					// R <远程节点>，L <本地节点>：都在区块10上
					// R：挖掘区块11，并将其传播到L
					// L：将区块11排队等待导入
					// L：注意到R的头和总难度相对于我们的链有所增加，开始同步
					// L：区块11的导入完成
					// L：开始同步，找到公共祖先在区块11上
					// L：请求从区块11开始的新标题（R的总难度更高，它必须有一些内容）
					// R：没有可提供的内容
					if mode != LightSync {
						head := d.blockchain.CurrentBlock()
						if !gotHeaders && td.Cmp(d.blockchain.GetTd(head.Hash(), head.Number.Uint64())) > 0 {
							return errStallingPeer
						}
					}
					// 如果进行快照或轻量级同步，请确保确实传递了承诺的标题。
					// 这是为了检测攻击者提供错误的轴点，然后放弃传递轴点后的区块，
					// 以标记无效内容的情况。对于完整导入，无法直接执行此检查，
					// 因为当标题下载完成时，区块可能仍在排队等待处理。
					// 然而，只要节点给了我们一些有用的东西，我们已经满意/取得了进展（上述检查）。
					if mode == SnapSync || mode == LightSync {
						head := d.lightchain.CurrentHeader()
						if td.Cmp(d.lightchain.GetTd(head.Hash(), head.Number.Uint64())) > 0 {
							return errStallingPeer
						}
					}
				}
				// Disable any rollback and return
				rollback = 0
				return nil
			}
			// 否则，将标题块分成批次并进行处理。
			headers, hashes := task.headers, task.hashes

			gotHeaders = true
			for len(headers) > 0 {
				// 如果在处理批次之间出现错误，请终止操作。
				select {
				case <-d.cancelCh:
					rollbackErr = errCanceled
					return errCanceled
				default:
				}
				// 选择下一个要导入的header block。
				limit := maxHeadersProcess
				if limit > len(headers) {
					limit = len(headers)
				}
				chunkHeaders := headers[:limit]
				chunkHashes := hashes[:limit]

				// 在仅同步header的情况下，立即验证标题块。
				if mode == SnapSync || mode == LightSync {
					// 尽管收到的标题可能全部有效，
					// 但传统的PoW/PoA同步不能接受合并后的标题。确保在此时拒绝任何过渡。
					var (
						rejected []*types.Header
						td       *big.Int
					)
					if !beaconMode && ttd != nil {
						td = d.blockchain.GetTd(chunkHeaders[0].ParentHash, chunkHeaders[0].Number.Uint64()-1)
						if td == nil {
							// 这实际上不应该发生，但现在先优雅地处理一下。
							log.Error("Failed to retrieve parent header TD", "number", chunkHeaders[0].Number.Uint64()-1, "hash", chunkHeaders[0].ParentHash)
							return fmt.Errorf("%w: parent TD missing", errInvalidChain)
						}
						for i, header := range chunkHeaders {
							td = new(big.Int).Add(td, header.Difficulty)
							if td.Cmp(ttd) >= 0 {
								// 达到了终止的总难度，允许导入最后一个header。
								if new(big.Int).Sub(td, header.Difficulty).Cmp(ttd) < 0 {
									chunkHeaders, rejected = chunkHeaders[:i+1], chunkHeaders[i+1:]
									if len(rejected) > 0 {
										// 生成更友好的用户日志，以指明第一个被真正拒绝的总难度。
										td = new(big.Int).Add(td, rejected[0].Difficulty)
									}
								} else {
									chunkHeaders, rejected = chunkHeaders[:i], chunkHeaders[i:]
								}
								break
							}
						}
					}
					if len(chunkHeaders) > 0 {
						if n, err := d.lightchain.InsertHeaderChain(chunkHeaders); err != nil {
							rollbackErr = err

							// 如果插入了一些标题，请将它们标记为不确定。
							if mode == SnapSync && n > 0 && rollback == 0 {
								rollback = chunkHeaders[0].Number.Uint64()
							}
							log.Warn("Invalid header encountered", "number", chunkHeaders[n].Number, "hash", chunkHashes[n], "parent", chunkHeaders[n].ParentHash, "err", err)
							return fmt.Errorf("%w: %v", errInvalidChain, err)
						}
						// 所有验证通过，跟踪在允许范围内的所有header。
						if mode == SnapSync {
							head := chunkHeaders[len(chunkHeaders)-1].Number.Uint64()
							if head-rollback > uint64(fsHeaderSafetyNet) {
								rollback = head - uint64(fsHeaderSafetyNet)
							} else {
								rollback = 1
							}
						}
					}
					if len(rejected) != 0 {
						// 达到合并阈值，停止导入，但不回滚。
						rollback = 0

						log.Info("Legacy sync reached merge threshold", "number", rejected[0].Number, "hash", rejected[0].Hash(), "td", td, "ttd", ttd)
						return ErrMergeTransition
					}
				}
				// 除非我们正在进行轻量级链同步，否则安排标题以进行关联内容的检索。
				if mode == FullSync || mode == SnapSync {
					// 如果达到了允许的挂起标题数量，请稍作等待。
					for d.queue.PendingBodies() >= maxQueuedHeaders || d.queue.PendingReceipts() >= maxQueuedHeaders {
						select {
						case <-d.cancelCh:
							rollbackErr = errCanceled
							return errCanceled
						case <-time.After(time.Second):
						}
					}
					// 否则，插入标题以进行内容检索。
					inserts := d.queue.Schedule(chunkHeaders, chunkHashes, origin)
					if len(inserts) != len(chunkHeaders) {
						rollbackErr = fmt.Errorf("stale headers: len inserts %v len(chunk) %v", len(inserts), len(chunkHeaders))
						return fmt.Errorf("%w: stale headers", errBadPeer)
					}
				}
				headers = headers[limit:]
				hashes = hashes[limit:]
				origin += uint64(limit)
			}
			// 如果找到更高的区块号码，则更新我们所知的最高区块号码。
			d.syncStatsLock.Lock()
			if d.syncStatsChainHeight < origin {
				d.syncStatsChainHeight = origin - 1
			}
			d.syncStatsLock.Unlock()

			// 通知内容下载器有新任务可用。
			for _, ch := range []chan bool{d.queue.blockWakeCh, d.queue.receiptWakeCh} {
				select {
				case ch <- true:
				default:
				}
			}
		}
	}
}   
```

## bodies处理
`fetchBodies`函数定义了一些闭包函数，~~然后调用了`fetchParts`函数~~ 现在调用的是`concurrentFetch`，`fetchParts`函数在`les`包当中
```go
// 迭代地下载预定的区块内容，利用任何可用的节点，为每个节点保留一块区块，
// 等待传输，并定期检查超时情况。
func (d *Downloader) fetchBodies(from uint64, beaconMode bool) error {
	log.Debug("Downloading block bodies", "origin", from)
	err := d.concurrentFetch((*bodyQueue)(d), beaconMode)

	log.Debug("Block body download terminated", "err", err)
	return err
}
```

concurrentFetch
```go
// 迭代地下载预定的区块部分，利用可用的节点，为每个节点保留一块区块的请求，并等待传输或超时。
func (d *Downloader) concurrentFetch(queue typedQueue, beaconMode bool) error {
	// Create a delivery channel to accept responses from all peers
	responses := make(chan *eth.Response)

	// Track the currently active requests and their timeout order
	pending := make(map[string]*eth.Request)
	defer func() {
		// Abort all requests on sync cycle cancellation. The requests may still
		// be fulfilled by the remote side, but the dispatcher will not wait to
		// deliver them since nobody's going to be listening.
		for _, req := range pending {
			req.Close()
		}
	}()
	ordering := make(map[*eth.Request]int)
	timeouts := prque.New[int64, *eth.Request](func(data *eth.Request, index int) {
		ordering[data] = index
	})

	timeout := time.NewTimer(0)
	if !timeout.Stop() {
		<-timeout.C
	}
	defer timeout.Stop()

	// Track the timed-out but not-yet-answered requests separately. We want to
	// keep tracking which peers are busy (potentially overloaded), so removing
	// all trace of a timed out request is not good. We also can't just cancel
	// the pending request altogether as that would prevent a late response from
	// being delivered, thus never unblocking the peer.
	stales := make(map[string]*eth.Request)
	defer func() {
		// Abort all requests on sync cycle cancellation. The requests may still
		// be fulfilled by the remote side, but the dispatcher will not wait to
		// deliver them since nobody's going to be listening.
		for _, req := range stales {
			req.Close()
		}
	}()
	// Subscribe to peer lifecycle events to schedule tasks to new joiners and
	// reschedule tasks upon disconnections. We don't care which event happened
	// for simplicity, so just use a single channel.
	peering := make(chan *peeringEvent, 64) // arbitrary buffer, just some burst protection

	peeringSub := d.peers.SubscribeEvents(peering)
	defer peeringSub.Unsubscribe()

	// Prepare the queue and fetch block parts until the block header fetcher's done
	finished := false
	for {
		// Short circuit if we lost all our peers
		if d.peers.Len() == 0 && !beaconMode {
			return errNoPeers
		}
		// If there's nothing more to fetch, wait or terminate
		if queue.pending() == 0 {
			if len(pending) == 0 && finished {
				return nil
			}
		} else {
			// Send a download request to all idle peers, until throttled
			var (
				idles []*peerConnection
				caps  []int
			)
			for _, peer := range d.peers.AllPeers() {
				pending, stale := pending[peer.id], stales[peer.id]
				if pending == nil && stale == nil {
					idles = append(idles, peer)
					caps = append(caps, queue.capacity(peer, time.Second))
				} else if stale != nil {
					if waited := time.Since(stale.Sent); waited > timeoutGracePeriod {
						// Request has been in flight longer than the grace period
						// permitted it, consider the peer malicious attempting to
						// stall the sync.
						peer.log.Warn("Peer stalling, dropping", "waited", common.PrettyDuration(waited))
						d.dropPeer(peer.id)
					}
				}
			}
			sort.Sort(&peerCapacitySort{idles, caps})

			var (
				progressed bool
				throttled  bool
				queued     = queue.pending()
			)
			for _, peer := range idles {
				// Short circuit if throttling activated or there are no more
				// queued tasks to be retrieved
				if throttled {
					break
				}
				if queued = queue.pending(); queued == 0 {
					break
				}
				// Reserve a chunk of fetches for a peer. A nil can mean either that
				// no more headers are available, or that the peer is known not to
				// have them.
				request, progress, throttle := queue.reserve(peer, queue.capacity(peer, d.peers.rates.TargetRoundTrip()))
				if progress {
					progressed = true
				}
				if throttle {
					throttled = true
					throttleCounter.Inc(1)
				}
				if request == nil {
					continue
				}
				// Fetch the chunk and make sure any errors return the hashes to the queue
				req, err := queue.request(peer, request, responses)
				if err != nil {
					// Sending the request failed, which generally means the peer
					// was disconnected in between assignment and network send.
					// Although all peer removal operations return allocated tasks
					// to the queue, that is async, and we can do better here by
					// immediately pushing the unfulfilled requests.
					queue.unreserve(peer.id) // TODO(karalabe): This needs a non-expiration method
					continue
				}
				pending[peer.id] = req

				ttl := d.peers.rates.TargetTimeout()
				ordering[req] = timeouts.Size()

				timeouts.Push(req, -time.Now().Add(ttl).UnixNano())
				if timeouts.Size() == 1 {
					timeout.Reset(ttl)
				}
			}
			// Make sure that we have peers available for fetching. If all peers have been tried
			// and all failed throw an error
			if !progressed && !throttled && len(pending) == 0 && len(idles) == d.peers.Len() && queued > 0 && !beaconMode {
				return errPeersUnavailable
			}
		}
		// Wait for something to happen
		select {
		case <-d.cancelCh:
			// If sync was cancelled, tear down the parallel retriever. Pending
			// requests will be cancelled locally, and the remote responses will
			// be dropped when they arrive
			return errCanceled

		case event := <-peering:
			// A peer joined or left, the tasks queue and allocations need to be
			// checked for potential assignment or reassignment
			peerid := event.peer.id

			if event.join {
				// Sanity check the internal state; this can be dropped later
				if _, ok := pending[peerid]; ok {
					event.peer.log.Error("Pending request exists for joining peer")
				}
				if _, ok := stales[peerid]; ok {
					event.peer.log.Error("Stale request exists for joining peer")
				}
				// Loop back to the entry point for task assignment
				continue
			}
			// A peer left, any existing requests need to be untracked, pending
			// tasks returned and possible reassignment checked
			if req, ok := pending[peerid]; ok {
				queue.unreserve(peerid) // TODO(karalabe): This needs a non-expiration method
				delete(pending, peerid)
				req.Close()

				if index, live := ordering[req]; live {
					timeouts.Remove(index)
					if index == 0 {
						if !timeout.Stop() {
							<-timeout.C
						}
						if timeouts.Size() > 0 {
							_, exp := timeouts.Peek()
							timeout.Reset(time.Until(time.Unix(0, -exp)))
						}
					}
					delete(ordering, req)
				}
			}
			if req, ok := stales[peerid]; ok {
				delete(stales, peerid)
				req.Close()
			}

		case <-timeout.C:
			// Retrieve the next request which should have timed out. The check
			// below is purely for to catch programming errors, given the correct
			// code, there's no possible order of events that should result in a
			// timeout firing for a non-existent event.
			req, exp := timeouts.Peek()
			if now, at := time.Now(), time.Unix(0, -exp); now.Before(at) {
				log.Error("Timeout triggered but not reached", "left", at.Sub(now))
				timeout.Reset(at.Sub(now))
				continue
			}
			// Stop tracking the timed out request from a timing perspective,
			// cancel it, so it's not considered in-flight anymore, but keep
			// the peer marked busy to prevent assigning a second request and
			// overloading it further.
			delete(pending, req.Peer)
			stales[req.Peer] = req

			timeouts.Pop() // Popping an item will reorder indices in `ordering`, delete after, otherwise will resurrect!
			if timeouts.Size() > 0 {
				_, exp := timeouts.Peek()
				timeout.Reset(time.Until(time.Unix(0, -exp)))
			}
			delete(ordering, req)

			// New timeout potentially set if there are more requests pending,
			// reschedule the failed one to a free peer
			fails := queue.unreserve(req.Peer)

			// Finally, update the peer's retrieval capacity, or if it's already
			// below the minimum allowance, drop the peer. If a lot of retrieval
			// elements expired, we might have overestimated the remote peer or
			// perhaps ourselves. Only reset to minimal throughput but don't drop
			// just yet.
			//
			// The reason the minimum threshold is 2 is that the downloader tries
			// to estimate the bandwidth and latency of a peer separately, which
			// requires pushing the measured capacity a bit and seeing how response
			// times reacts, to it always requests one more than the minimum (i.e.
			// min 2).
			peer := d.peers.Peer(req.Peer)
			if peer == nil {
				// If the peer got disconnected in between, we should really have
				// short-circuited it already. Just in case there's some strange
				// codepath, leave this check in not to crash.
				log.Error("Delivery timeout from unknown peer", "peer", req.Peer)
				continue
			}
			if fails > 2 {
				queue.updateCapacity(peer, 0, 0)
			} else {
				d.dropPeer(peer.id)

				// If this peer was the master peer, abort sync immediately
				d.cancelLock.RLock()
				master := peer.id == d.cancelPeer
				d.cancelLock.RUnlock()

				if master {
					d.cancel()
					return errTimeout
				}
			}

		case res := <-responses:
			// Response arrived, it may be for an existing or an already timed
			// out request. If the former, update the timeout heap and perhaps
			// reschedule the timeout timer.
			index, live := ordering[res.Req]
			if live {
				timeouts.Remove(index)
				if index == 0 {
					if !timeout.Stop() {
						<-timeout.C
					}
					if timeouts.Size() > 0 {
						_, exp := timeouts.Peek()
						timeout.Reset(time.Until(time.Unix(0, -exp)))
					}
				}
				delete(ordering, res.Req)
			}
			// Delete the pending request (if it still exists) and mark the peer idle
			delete(pending, res.Req.Peer)
			delete(stales, res.Req.Peer)

			// Signal the dispatcher that the round trip is done. We'll drop the
			// peer if the data turns out to be junk.
			res.Done <- nil
			res.Req.Close()

			// If the peer was previously banned and failed to deliver its pack
			// in a reasonable time frame, ignore its message.
			if peer := d.peers.Peer(res.Req.Peer); peer != nil {
				// Deliver the received chunk of data and check chain validity
				accepted, err := queue.deliver(peer, res)
				if errors.Is(err, errInvalidChain) {
					return err
				}
				// Unless a peer delivered something completely else than requested (usually
				// caused by a timed out request which came through in the end), set it to
				// idle. If the delivery's stale, the peer should have already been idled.
				if !errors.Is(err, errStaleDelivery) {
					queue.updateCapacity(peer, accepted, res.Time)
				}
			}

		case cont := <-queue.waker():
			// The header fetcher sent a continuation flag, check if it's done
			if !cont {
				finished = true
			}
		}
	}
}
```

























































