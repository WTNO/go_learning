queue给downloader提供了调度功能和限流的功能。 通过调用Schedule/ScheduleSkeleton来申请对任务进行调度，然后调用ReserveXXX方法来领取调度完成的任务，并在downloader里面的线程来执行，调用DeliverXXX方法把下载完的数据给queue。 最后通过WaitResults来获取已经完成的任务。中间还有一些对任务的额外控制，ExpireXXX用来控制任务是否超时， CancelXXX用来取消任务。

## Schedule方法
Schedule调用申请对一些区块头进行下载调度。可以看到做了一些合法性检查之后，把任务插入了blockTaskPool，blockTaskQueue，receiptTaskPool，receiptTaskQueue。 TaskPool是Map，用来记录header的hash是否存在。 TaskQueue是优先级队列，优先级是区块的高度的负数， 这样区块高度越小的优先级越高，就实现了首先调度小的任务的功能。
```go
// Schedule adds a set of headers for the download queue for scheduling, returning
// the new headers encountered.
// from表示headers里面第一个元素的区块高度。 返回值返回了所有被接收的header
func (q *queue) Schedule(headers []*types.Header, hashes []common.Hash, from uint64) []*types.Header {
	q.lock.Lock()
	defer q.lock.Unlock()

	// Insert all the headers prioritised by the contained block number
	inserts := make([]*types.Header, 0, len(headers))
	for i, header := range headers {
		// Make sure chain order is honoured and preserved throughout
		hash := hashes[i]
		// header.Number:此区块高度。用于对区块标注序号，在一条区块链上，区块高度必须是连续递增
		if header.Number == nil || header.Number.Uint64() != from {
			log.Warn("Header broke chain ordering", "number", header.Number, "hash", hash, "expected", from)
			break
		}
		// headerHead存储了最后一个插入的区块头， 检查当前区块是否正确的链接。
		if q.headerHead != (common.Hash{}) && q.headerHead != header.ParentHash {
			log.Warn("Header broke chain ancestry", "number", header.Number, "hash", hash)
			break
		}
		// 确保不执行重复的请求，即使区块为空也不能跳过，因为这会触发fetchResult的创建。
		// 检查重复，这里直接continue了，那不是from对不上了。
		if _, ok := q.blockTaskPool[hash]; ok {
			log.Warn("Header already scheduled for block fetch", "number", header.Number, "hash", hash)
		} else {
			q.blockTaskPool[hash] = header
			q.blockTaskQueue.Push(header, -int64(header.Number.Uint64()))
		}
		// Queue for receipt retrieval
		if q.mode == SnapSync && !header.EmptyReceipts() {
			if _, ok := q.receiptTaskPool[hash]; ok {
				log.Warn("Header already scheduled for receipt fetch", "number", header.Number, "hash", hash)
			} else {
				q.receiptTaskPool[hash] = header
				q.receiptTaskQueue.Push(header, -int64(header.Number.Uint64()))
			}
		}
		inserts = append(inserts, header)
		q.headerHead = hash
		from++
	}
	return inserts
}
```
> blockTaskQueue、receiptTaskQueue是Prque类型，通过堆实现了入堆排序功能，这里是以区块的高度的负数为优先级来排序， 这样区块高度越小的优先级越高，就实现了首先调度小的任务的功能。

## ReserveXXX
ReserveXXX方法用来从queue里面领取一些任务来执行。downloader里面的goroutine会调用这个方法来领取一些任务来执行。 这个方法直接调用了reserveHeaders方法。 所有的ReserveXXX方法都会调用reserveHeaders方法，除了传入的参数有一些区别。
```go
// 为给定的节点保留一组区块内容的获取请求，跳过任何先前失败的下载。
// 除了下一批需要获取的内容之外，它还返回一个标志，指示是否排队了空的区块需要进行处理。
func (q *queue) ReserveBodies(p *peerConnection, count int) (*fetchRequest, bool, bool) {
	q.lock.Lock()
	defer q.lock.Unlock()

	return q.reserveHeaders(p, count, q.blockTaskPool, q.blockTaskQueue, q.blockPendPool, bodyType)
}
```

reserveHeaders
```go
// 为给定的节点保留一组数据下载操作，跳过任何先前失败的操作。
// 该方法是一个通用版本，被单独的特殊保留函数使用。
// 
// 注意，该方法期望队列锁已经被持有以进行写操作。
// 
//，所以它们已经需要一个锁。
//
// Returns:
//
//	item     - the fetchRequest
//	progress - whether any progress was made
//	throttle - if the caller should throttle for a while
// 这个方法调用的时候，假设已经获取到锁，这个方法里面没有锁的原因是参数已经传入到函数里面了，所以调用的时候就需要获取锁。
func (q *queue) reserveHeaders(p *peerConnection, count int, taskPool map[common.Hash]*types.Header, taskQueue *prque.Prque[int64, *types.Header],
	pendPool map[string]*fetchRequest, kind uint) (*fetchRequest, bool, bool) {
	// 如果连接池已经耗尽，或者节点已经在下载其他内容（为了确保状态不被破坏），则进行短路处理。
	if taskQueue.Empty() {
		return nil, false, true
	}
	// 如果这个peer还有下载任务没有完成
	if _, ok := pendPool[p.id]; ok {
		return nil, false, false
	}
	// 获取一批任务，跳过先前失败的任务。
	send := make([]*types.Header, 0, count)
	skip := make([]*types.Header, 0)
	progress := false
	throttled := false
	for proc := 0; len(send) < count && !taskQueue.Empty(); proc++ {
		// 任务队列将按顺序弹出项目，因此最高优先级的区块也是最低的区块编号。
		header, _ := taskQueue.Peek()

		// 我们可以向结果缓存询问该标题是否在“优先级”区块段内。如果不在，则需要进行限制。
        // AddFetch用于添加一个用于获取区块内容/收据的标题。当队列想要保留标题以进行获取时使用。
		stale, throttle, item, err := q.resultCache.AddFetch(header, q.mode == SnapSync)
		if stale {
			// 不要将此项放回任务队列，该项已经被传递给上游。
			taskQueue.PopItem()
			progress = true
			delete(taskPool, header.Hash())
			proc = proc - 1
			log.Error("Fetch reservation already delivered", "number", header.Number.Uint64())
			continue
		}
		if throttle {
			// 没有可用的结果槽。将其保留在任务队列中。
			// 但是，如果有任何被标记为“跳过”的结果槽，我们不应告诉调用者进行限制，
			// 因为我们仍希望其他节点为我们获取这些结果。
			throttled = len(skip) == 0
			break
		}
		if err != nil {
			// 这绝对不应该发生。
			log.Warn("Failed to reserve headers", "err", err)
			// 没有可用的结果槽。将其保留在任务队列中。
			break
		}
		if item.Done(kind) {
			// 如果这是一个空操作，则可以跳过此任务。
			delete(taskPool, header.Hash())
			taskQueue.PopItem()
			proc = proc - 1
			progress = true
			continue
		}
		// 将其从任务队列中移除。
		taskQueue.PopItem()
		// 否则，除非已知该节点没有这些数据，将其添加到检索列表中。
		// Lacks代表节点之前明确表示过没有这个hash的数据。
		if p.Lacks(header.Hash()) {
			skip = append(skip, header)
		} else {
			send = append(send, header)
		}
	}
	// 将所有跳过的headers合并回taskQueue
	for _, header := range skip {
		taskQueue.Push(header, -int64(header.Number.Uint64()))
	}
	// resultCache为已下载但尚未传递的获取结果。
	// 如果有可处理的项目可用，则HasCompletedItems返回true
	if q.resultCache.HasCompletedItems() {
		// Wake Results, resultCache was modified
		q.active.Signal()
	}
	// 组装并返回区块下载请求。
	if len(send) == 0 {
		return nil, progress, throttled
	}
	request := &fetchRequest{
		Peer:    p,
		Headers: send,
		Time:    time.Now(),
	}
	pendPool[p.id] = request
	return request, progress, throttled
}
```

ReserveReceipts 可以看到和ReserveBodys差不多。不过是队列换了而已。
```go
// 为给定的节点保留一组收据获取请求，跳过任何先前失败的下载。
// 除了下一批需要获取的收据之外，它还返回一个标志，指示是否排队了空的收据需要导入。
func (q *queue) ReserveReceipts(p *peerConnection, count int) (*fetchRequest, bool, bool) {
	q.lock.Lock()
	defer q.lock.Unlock()

	return q.reserveHeaders(p, count, q.receiptTaskPool, q.receiptTaskQueue, q.receiptPendPool, receiptType)
}

```

## DeliverXXX
Deliver方法在数据下载完之后会被调用。
```go
// DeliverBodies将区块内容获取的响应注入到results队列中。
// 该方法返回从传递中接受的区块内容的数量，并唤醒等待数据传递的线程。
func (q *queue) DeliverBodies(id string, txLists [][]*types.Transaction, txListHashes []common.Hash,
	uncleLists [][]*types.Header, uncleListHashes []common.Hash,
	withdrawalLists [][]*types.Withdrawal, withdrawalListHashes []common.Hash) (int, error) {
	q.lock.Lock()
	defer q.lock.Unlock()

	validate := func(index int, header *types.Header) error {
		if txListHashes[index] != header.TxHash {
			return errInvalidBody
		}
		if uncleListHashes[index] != header.UncleHash {
			return errInvalidBody
		}
		if header.WithdrawalsHash == nil {
			// nil hash means that withdrawals should not be present in body
			// 空的哈希意味着在区块内容中不应该有提款。
			if withdrawalLists[index] != nil {
				return errInvalidBody
			}
		} else { 
			// non-nil hash: body must have withdrawals
			// 非空的哈希：区块内容必须包含提款。
			if withdrawalLists[index] == nil {
				return errInvalidBody
			}
			if withdrawalListHashes[index] != *header.WithdrawalsHash {
				return errInvalidBody
			}
		}
		// 区块必须具有与标题燃气使用量相对应的一定数量的区块数据块，在Cancun硬分叉之前为零。
		var blobs int
		for _, tx := range txLists[index] {
			// 计算区块数据块的数量以与标题的dataGasUsed进行验证。dataGasUsed
			blobs += len(tx.BlobHashes())

			// Validate the data blobs individually too
			// 要逐个验证数据块。
			if tx.Type() == types.BlobTxType {
				if len(tx.BlobHashes()) == 0 {
					return errInvalidBody
				}
				for _, hash := range tx.BlobHashes() {
					if hash[0] != params.BlobTxHashVersion {
						return errInvalidBody
					}
				}
			}
		}
		if header.DataGasUsed != nil {
			if want := *header.DataGasUsed / params.BlobTxDataGasPerBlob; uint64(blobs) != want { // 除法是因为header肯定是正确的，而区块内容可能会过于臃肿。
				return errInvalidBody
			}
		} else {
			if blobs != 0 {
				return errInvalidBody
			}
		}
		return nil
	}

	reconstruct := func(index int, result *fetchResult) {
		result.Transactions = txLists[index]
		result.Uncles = uncleLists[index]
		result.Withdrawals = withdrawalLists[index]
		result.SetBodyDone()
	}
	return q.deliver(id, q.blockTaskPool, q.blockTaskQueue, q.blockPendPool,
		bodyReqTimer, bodyInMeter, bodyDropMeter, len(txLists), validate, reconstruct)
}
```

deliver方法
```go
// deliver方法将数据检索响应注入到结果队列中。
//
// 请注意，该方法期望队列锁已经被写入持有。
// 之所以在这里不获取锁是因为参数已经需要访问队列，所以它们已经需要锁定。
func (q *queue) deliver(id string, taskPool map[common.Hash]*types.Header,
	taskQueue *prque.Prque[int64, *types.Header], pendPool map[string]*fetchRequest,
	reqTimer metrics.Timer, resInMeter metrics.Meter, resDropMeter metrics.Meter,
	results int, validate func(index int, header *types.Header) error,
	reconstruct func(index int, result *fetchResult)) (int, error) {
	// Short circuit if the data was never requested
	// 检查数据是否从来没有请求过。
	request := pendPool[id]
	if request == nil {
		resDropMeter.Mark(int64(results))
		return 0, errNoFetchesPending
	}
	delete(pendPool, id)

	reqTimer.UpdateSince(request.Time)
	resInMeter.Mark(int64(results))

	// 如果没有检索到数据项，则将其标记为对原始节点不可用。
	if results == 0 {
		// 如果结果为空。 那么标识这个peer没有这些数据。
		for _, header := range request.Headers {
			request.Peer.MarkLacking(header.Hash())
		}
	}
	// 将每个结果与其标头和已检索的数据部分组装在一起。
	var (
		accepted int
		failure  error
		i        int
		hashes   []common.Hash
	)
	for _, header := range request.Headers {
		// Short circuit assembly if no more fetch results are found
		if i >= results {
			break
		}
		// Validate the fields
		if err := validate(i, header); err != nil {
			failure = err
			break
		}
		hashes = append(hashes, header.Hash())
		i++
	}

	for _, header := range request.Headers[:i] {
		if res, stale, err := q.resultCache.GetDeliverySlot(header.Number.Uint64()); err == nil && !stale {
			reconstruct(accepted, res)
		} else {
			// else: 在此处和上述之间，其他节点填充了此结果，或者确实是一个无操作。这不应该发生，但如果发生了，也不是什么值得恐慌的事情。
			log.Error("Delivery stale", "stale", stale, "number", header.Number.Uint64(), "err", err)
			failure = errStaleDelivery
		}
		// Clean up a successful fetch
		delete(taskPool, hashes[accepted])
		accepted++
	}
	resDropMeter.Mark(int64(results - accepted))

	// Return all failed or missing fetches to the queue
	// 所有没有成功的请求加入taskQueue
	for _, header := range request.Headers[accepted:] {
		taskQueue.Push(header, -int64(header.Number.Uint64()))
	}
	// Wake up Results
	// 如果结果有变更，通知WaitResults线程启动。
	if accepted > 0 {
		q.active.Signal()
	}
	if failure == nil {
		return accepted, nil
	}
	// 如果没有任何数据是有效的，则是一个陈旧的传递。
	if accepted > 0 {
		return accepted, fmt.Errorf("partial failure: %v", failure)
	}
	return accepted, fmt.Errorf("%w: %v", failure, errStaleDelivery)
}
```

## ExpireXXX and CancelXXX
### ExpireXXX
ExpireBodies函数获取了锁，然后直接调用了expire函数。
```go
// 检查超过超时限制的正在进行中的块主体请求，取消它们并返回负责进行惩罚的节点。
func (q *queue) ExpireBodies(peer string) int {
	q.lock.Lock()
	defer q.lock.Unlock()

	bodyTimeoutMeter.Mark(1)
	return q.expire(peer, q.blockPendPool, q.blockTaskQueue)
}
```

expire函数
```go
// expire是一个通用的检查方法，将特定的过期任务从待处理池移回任务池。
// 传递给taskQueue的语法有些奇怪，因为我们需要一个通用的expire方法来处理两种类型，
// 但目前至少还不支持（Go 1.19）。
// 
// 请注意，该方法期望队列锁已经被持有。
// 之所以在此处不获取锁是因为参数已经需要访问队列，所以它们已经需要锁定。
func (q *queue) expire(peer string, pendPool map[string]*fetchRequest, taskQueue interface{}) int {
	// 获取被过期的请求并在其不存在时记录错误，因为没有任何事件顺序应该导致这种过期。
	req := pendPool[peer]
	if req == nil {
		log.Error("Expired request does not exist", "peer", peer)
		return 0
	}
	delete(pendPool, peer)

	// Return any non-satisfied requests to the pool
	if req.From > 0 {
		taskQueue.(*prque.Prque[int64, uint64]).Push(req.From, -int64(req.From))
	}
	for _, header := range req.Headers {
		taskQueue.(*prque.Prque[int64, *types.Header]).Push(header, -int64(header.Number.Uint64()))
	}
	return len(req.Headers)
}
```

## CancelXXX
Cancle函数取消已经分配的任务， 把任务重新加入到任务池。

> 当前版本的`eth/downloader/queue.go`中不存在CancelXXX方法，仅存在于`les/downloader/queue.go`
```go
// 中止一个主体获取请求，并将所有待处理的标头返回到任务队列中。
func (q *queue) CancelBodies(request *fetchRequest) {
	q.lock.Lock()
	defer q.lock.Unlock()
	q.cancel(request, q.blockTaskQueue, q.blockPendPool)
}

// 中止一个获取请求，并将所有待处理的哈希返回到任务队列中。
func (q *queue) cancel(request *fetchRequest, taskQueue interface{}, pendPool map[string]*fetchRequest) {
    if request.From > 0 {
        taskQueue.(*prque.Prque[int64, uint64]).Push(request.From, -int64(request.From))
    }
    for _, header := range request.Headers {
        taskQueue.(*prque.Prque[int64, *types.Header]).Push(header, -int64(header.Number.Uint64()))
    }
    delete(pendPool, request.Peer.id)
}
```

## ScheduleSkeleton
Schedule方法传入的是已经fetch好的header。Schedule(headers []*types.Header, from uint64)。而ScheduleSkeleton函数的参数是一个骨架， 然后请求对骨架进行填充。所谓的骨架是指我首先每隔192个区块请求一个区块头，然后把返回的header传入ScheduleSkeleton。 在Schedule函数中只需要queue调度区块体和回执的下载，而在ScheduleSkeleton函数中，还需要调度那些缺失的区块头的下载。
```go
// ScheduleSkeleton函数将一批header retrieval tasks添加到队列中，以填充an already retrieved header skeleton
func (q *queue) ScheduleSkeleton(from uint64, skeleton []*types.Header) {
	q.lock.Lock()
	defer q.lock.Unlock()

	// No skeleton retrieval can be in progress, fail hard if so (huge implementation bug)
	// 如果存在正在进行的标头框架检索，将会失败（这是一个严重的实现错误）。
	if q.headerResults != nil {
		panic("skeleton assembly already in progress")
	}
	// Schedule all the header retrieval tasks for the skeleton assembly
	// 因为这个方法在skeleton为false的时候不会调用。 所以一些初始化工作放在这里执行。
	q.headerTaskPool = make(map[uint64]*types.Header)
	q.headerTaskQueue = prque.New[int64, uint64](nil)
	q.headerPeerMiss = make(map[string]map[uint64]struct{}) // Reset availability to correct invalid chains
	q.headerResults = make([]*types.Header, len(skeleton)*MaxHeaderFetch)
	q.headerHashes = make([]common.Hash, len(skeleton)*MaxHeaderFetch)
	q.headerProced = 0
	q.headerOffset = from
	q.headerContCh = make(chan bool, 1)

	for i, header := range skeleton {
		index := from + uint64(i*MaxHeaderFetch)

		q.headerTaskPool[index] = header
		q.headerTaskQueue.Push(index, -int64(index))
	}
}
```

### ReserveHeaders
这个方法只skeleton的模式下才会被调用。 用来给peer保留fetch 区块头的任务。
```go
// RetrieveHeaders retrieves the header chain assemble based on the scheduled
// skeleton.
// 根据预定的标头框架检索标头链组装
func (q *queue) RetrieveHeaders() ([]*types.Header, []common.Hash, int) {
	q.lock.Lock()
	defer q.lock.Unlock()

	headers, hashes, proced := q.headerResults, q.headerHashes, q.headerProced
	q.headerResults, q.headerHashes, q.headerProced = nil, nil, 0

	return headers, hashes, proced
}
```

### DeliverHeaders
```go
// 将一个头部检索响应注入到头部结果缓存中。
// 这个方法对于所有的区块头，要么全部接收，要么全部拒绝(如果不能映射到一个skeleton上面)
// 
// 如果区块头被接收，这个方法会试图把他们投递到headerProcCh管道上面。
// 不过这个方法不会阻塞式的投递。而是尝试投递，如果不能投递就返回。
func (q *queue) DeliverHeaders(id string, headers []*types.Header, hashes []common.Hash, headerProcCh chan *headerTask) (int, error) {
	q.lock.Lock()
	defer q.lock.Unlock()

	var logger log.Logger
	if len(id) < 16 {
		// Tests use short IDs, don't choke on them
		logger = log.New("peer", id)
	} else {
		logger = log.New("peer", id[:16])
	}
	// Short circuit if the data was never requested
	request := q.headerPendPool[id]
	if request == nil {
		headerDropMeter.Mark(int64(len(headers)))
		return 0, errNoFetchesPending
	}
	delete(q.headerPendPool, id)

	headerReqTimer.UpdateSince(request.Time)
	headerInMeter.Mark(int64(len(headers)))

	// Ensure headers can be mapped onto the skeleton chain
	target := q.headerTaskPool[request.From].Hash()

	accepted := len(headers) == MaxHeaderFetch
	// 首先长度需要匹配， 然后检查区块号和最后一块区块的Hash值是否能够对应上。
	if accepted {
		if headers[0].Number.Uint64() != request.From {
			logger.Trace("First header broke chain ordering", "number", headers[0].Number, "hash", hashes[0], "expected", request.From)
			accepted = false
		} else if hashes[len(headers)-1] != target {
			logger.Trace("Last header broke skeleton structure ", "number", headers[len(headers)-1].Number, "hash", hashes[len(headers)-1], "expected", target)
			accepted = false
		}
	}
	// 依次检查每一块区块的区块号， 以及链接是否正确。
	if accepted {
		parentHash := hashes[0]
		for i, header := range headers[1:] {
			hash := hashes[i+1]
			if want := request.From + 1 + uint64(i); header.Number.Uint64() != want {
				logger.Warn("Header broke chain ordering", "number", header.Number, "hash", hash, "expected", want)
				accepted = false
				break
			}
			if parentHash != header.ParentHash {
				logger.Warn("Header broke chain ancestry", "number", header.Number, "hash", hash)
				accepted = false
				break
			}
			// Set-up parent hash for next round
			parentHash = hash
		}
	}
	// If the batch of headers wasn't accepted, mark as unavailable
	// 如果不被接收，那么标记这个peer在这个任务上的失败。下次请求就不会投递给这个peer
	if !accepted {
		logger.Trace("Skeleton filling not accepted", "from", request.From)
		headerDropMeter.Mark(int64(len(headers)))

		miss := q.headerPeerMiss[id]
		if miss == nil {
			q.headerPeerMiss[id] = make(map[uint64]struct{})
			miss = q.headerPeerMiss[id]
		}
		miss[request.From] = struct{}{}

		q.headerTaskQueue.Push(request.From, -int64(request.From))
		return 0, errors.New("delivery not accepted")
	}
	// Clean up a successful fetch and try to deliver any sub-results
	copy(q.headerResults[request.From-q.headerOffset:], headers)
	copy(q.headerHashes[request.From-q.headerOffset:], hashes)

	delete(q.headerTaskPool, request.From)

	ready := 0
	// 计算这次到来的header可以让headerResults有多少数据可以投递了。
	for q.headerProced+ready < len(q.headerResults) && q.headerResults[q.headerProced+ready] != nil {
		ready += MaxHeaderFetch
	}
	if ready > 0 {
		// Headers are ready for delivery, gather them and push forward (non blocking)
		processHeaders := make([]*types.Header, ready)
		copy(processHeaders, q.headerResults[q.headerProced:q.headerProced+ready])

		processHashes := make([]common.Hash, ready)
		copy(processHashes, q.headerHashes[q.headerProced:q.headerProced+ready])

		select {
		// 尝试投递
		case headerProcCh <- &headerTask{
			headers: processHeaders,
			hashes:  processHashes,
		}:
			logger.Trace("Pre-scheduled new headers", "count", len(processHeaders), "from", processHeaders[0].Number)
			q.headerProced += len(processHeaders)
		default:
		}
	}
	// Check for termination and return
	if len(q.headerTaskPool) == 0 {
		// 这个通道比较重要， 如果这个通道接收到数据，说明所有的header任务已经完成。
		q.headerContCh <- false
	}
	return len(headers), nil
}
```

### RetrieveHeaders
ScheduleSkeleton函数在上次调度还没有做完的情况下是不会调用的。 所以上次调用完成之后，会使用这个方法来获取结果，重置状态。
```go
// RetrieveHeaders retrieves the header chain assemble based on the scheduled
// skeleton.
func (q *queue) RetrieveHeaders() ([]*types.Header, []common.Hash, int) {
	q.lock.Lock()
	defer q.lock.Unlock()

	headers, hashes, proced := q.headerResults, q.headerHashes, q.headerProced
	q.headerResults, q.headerHashes, q.headerProced = nil, nil, 0

	return headers, hashes, proced
}
```